{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rGr3Aw-Iq9fQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61dcf49e-13ee-4cf9-9a3f-284b7dd9604b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import string\n",
        "import time\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from pathlib import Path\n",
        "\n",
        "from transformers import AutoTokenizer, AdamW, BertForQuestionAnswering, AutoModel"
      ],
      "metadata": {
        "id": "h-3yF6VuiaRC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Tokenizer & Load Model"
      ],
      "metadata": {
        "id": "Taroc_34i-C7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_CHECKPOINT = \"fahmiaziz/bert-squad-v2\""
      ],
      "metadata": {
        "id": "4szljSBZiaOP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "model = BertForQuestionAnswering.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3SDIuFMiaK9",
        "outputId": "c5c80521-7031-434a-9707-f5c2fbe256a8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForQuestionAnswering(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility Functions"
      ],
      "metadata": {
        "id": "rmeKKzzukDgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(context: str, query: str):\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        query, context, return_tensors=\"pt\"\n",
        "    )\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    answer_start = torch.argmax(outputs[0])   # get the most likely beginning of answer with the argmax of the score\n",
        "    answer_end = torch.argmax(outputs[1]) + 1\n",
        "    answer = tokenizer.convert_tokens_to_string(\n",
        "        tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end])\n",
        "    )\n",
        "\n",
        "    return answer\n",
        "\n",
        "def normalize_text(s):\n",
        "    # remove article (a, an, the)\n",
        "    s = re.sub(r'\\b(a|an|the)\\b', ' ', s, flags=re.UNICODE)\n",
        "\n",
        "    # remove punctuation\n",
        "    exclude = set(string.punctuation)\n",
        "    s = ''.join(ch for ch in s if ch not in exclude)\n",
        "\n",
        "    # Mengubah teks menjadi huruf kecil dan menghilangkan spasi berlebih\n",
        "    s = \" \".join(s.split()).lower()\n",
        "\n",
        "    return s\n"
      ],
      "metadata": {
        "id": "twfoBHnZiaHh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_exact_match(prediction, truth):\n",
        "    return int(normalize_text(prediction) == normalize_text(truth))\n",
        "\n",
        "\n",
        "def compute_f1(prediction, truth):\n",
        "    pred_tokens = normalize_text(prediction).split()\n",
        "    truth_tokens = normalize_text(truth).split()\n",
        "\n",
        "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "      return int(pred_tokens == truth_tokens)\n",
        "\n",
        "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "\n",
        "    # if there are no common tokens then f1 = 0\n",
        "    if len(common_tokens) == 0:\n",
        "      return 0\n",
        "\n",
        "    prec = len(common_tokens) / len(pred_tokens)\n",
        "    rec = len(common_tokens) / len(truth_tokens)\n",
        "\n",
        "    return 2 * (prec * rec) / (prec + rec)\n",
        "\n",
        "def compute_precision(prediction, truth):\n",
        "    pred_tokens = normalize_text(prediction).split()\n",
        "    truth_tokens = normalize_text(truth).split()\n",
        "\n",
        "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "      return int(pred_tokens == truth_tokens)\n",
        "\n",
        "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "\n",
        "    # if there are no common tokens then f1 = 0\n",
        "    if len(common_tokens) == 0:\n",
        "      return 0\n",
        "\n",
        "    prec = len(common_tokens) / len(pred_tokens)\n",
        "    rec = len(common_tokens) / len(truth_tokens)\n",
        "\n",
        "    return prec\n",
        "\n",
        "def compute_recall(prediction, truth):\n",
        "    pred_tokens = normalize_text(prediction).split()\n",
        "    truth_tokens = normalize_text(truth).split()\n",
        "\n",
        "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "      return int(pred_tokens == truth_tokens)\n",
        "\n",
        "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "\n",
        "    # if there are no common tokens then f1 = 0\n",
        "    if len(common_tokens) == 0:\n",
        "      return 0\n",
        "\n",
        "    prec = len(common_tokens) / len(pred_tokens)\n",
        "    rec = len(common_tokens) / len(truth_tokens)\n",
        "\n",
        "    return rec"
      ],
      "metadata": {
        "id": "XDs5ENuViaEY"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def give_an_answer(context,query,answer):\n",
        "  prediction = predict(context,query)\n",
        "  em_score = compute_exact_match(prediction, answer)\n",
        "  f1_score = compute_f1(prediction, answer)\n",
        "  prec = compute_precision(prediction, answer)\n",
        "  rec = compute_recall(prediction, answer)\n",
        "\n",
        "  print(f\"Question: {query}\")\n",
        "  print(f\"Prediction: {prediction}\")\n",
        "  print(f\"True Answer: {answer}\")\n",
        "  print(f\"EM: {em_score}\")\n",
        "  print(f\"F1: {f1_score}\")\n",
        "  print(f\"Precision: {prec}\")\n",
        "  print(f\"Recall: {rec}\")\n",
        "\n",
        "  return f1_score"
      ],
      "metadata": {
        "id": "mYA3KlhgiZ_K"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"squad_v2\").shuffle(100)"
      ],
      "metadata": {
        "id": "0HXVD8BhiZ7Q"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"validation\"][100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHUWP3vRiZ2m",
        "outputId": "d834e127-93d1-4b1c-b8e9-de603d1690c6"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '5ad14d8e645df0001a2d16b9',\n",
              " 'title': 'European_Union_law',\n",
              " 'context': 'The Social Charter was subsequently adopted in 1989 by 11 of the then 12 member states. The UK refused to sign the Social Charter and was exempt from the legislation covering Social Charter issues unless it agreed to be bound by the legislation. The UK subsequently was the only member state to veto the Social Charter being included as the \"Social Chapter\" of the 1992 Maastricht Treaty - instead, an Agreement on Social Policy was added as a protocol. Again, the UK was exempt from legislation arising from the protocol, unless it agreed to be bound by it. The protocol was to become known as \"Social Chapter\", despite not actually being a chapter of the Maastricht Treaty. To achieve aims of the Agreement on Social Policy the European Union was to \"support and complement\" the policies of member states. The aims of the Agreement on Social Policy are:',\n",
              " 'question': 'Who was the only member state not to veto the Social Charter the Social Charter being included as the Social Charter of Masstricht Treaty?',\n",
              " 'answers': {'text': [], 'answer_start': []}}"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question Answering"
      ],
      "metadata": {
        "id": "tSzqi0hNpPkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Batman is a superhero who appears in American comic books published by DC Comics. The character was created by artist Bob Kane and writer Bill Finger, and debuted in the 27th issue of the comic book Detective Comics on March 30, 1939. In the DC Universe continuity, Batman is the alias of Bruce Wayne, a wealthy American playboy, philanthropist, and industrialist who resides in Gotham City.\"\n",
        "\n",
        "queries = [\"In which comics does Batman appear?\", \"Who created the character?\", \"When did Batman debut?\"]\n",
        "\n",
        "answers = [\"DC Comics\", \"Bob Kane\", \"March 30, 1939\"]\n",
        "\n",
        "for q,a in zip(queries,answers):\n",
        "  give_an_answer(context,q,a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2d4UaKHiZvI",
        "outputId": "6fab4e7b-53ad-446d-b8bd-9dc5f6a1fbc7"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: In which comics does Batman appear?\n",
            "Prediction: detective comics\n",
            "True Answer: DC Comics\n",
            "EM: 0\n",
            "F1: 0.5\n",
            "Precision: 0.5\n",
            "Recall: 0.5\n",
            "Question: Who created the character?\n",
            "Prediction: bob kane\n",
            "True Answer: Bob Kane\n",
            "EM: 1\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "Question: When did Batman debut?\n",
            "Prediction: march 30, 1939.\n",
            "True Answer: March 30, 1939\n",
            "EM: 1\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 200 Questions from the squad-v2 validation set"
      ],
      "metadata": {
        "id": "2q5zu32i67c2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download data\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB3ysASe5L0P",
        "outputId": "31826343-0f02-4b48-ac99-45b1c0573e7a"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-13 12:22:35--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4370528 (4.2M) [application/json]\n",
            "Saving to: ‘dev-v2.0.json’\n",
            "\n",
            "\rdev-v2.0.json         0%[                    ]       0  --.-KB/s               \rdev-v2.0.json       100%[===================>]   4.17M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-10-13 12:22:35 (229 MB/s) - ‘dev-v2.0.json’ saved [4370528/4370528]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = Path('/content/dev-v2.0.json')\n",
        "\n",
        "# Open .json file\n",
        "with open(path, 'rb') as f:\n",
        "    dev_dict = json.load(f)\n"
      ],
      "metadata": {
        "id": "2sIGJOOQ_a9D"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "test_contexts  = []\n",
        "test_questions = []\n",
        "test_answers   = []\n",
        "TOTAL_QA = 100\n",
        "\n",
        "# Search for context, question and answer in each passage and append to respective lists\n",
        "for group in dev_dict['data']:\n",
        "    for passage in group['paragraphs']:\n",
        "        context = passage['context']\n",
        "        for qa in passage['qas']:\n",
        "            question = qa['question']\n",
        "            for answer in qa['answers']:\n",
        "                test_contexts.append(context)\n",
        "                test_questions.append(question)\n",
        "                test_answers.append(answer)\n",
        "\n",
        "# Store information triplets in dictionaries and append to a list to randomize\n",
        "triplets = []\n",
        "for c,q,a in zip(test_contexts, test_questions, test_answers):\n",
        "    instance = {}\n",
        "    instance['context'] = c\n",
        "    instance['question'] = q\n",
        "    instance['answer'] = a\n",
        "    triplets.append(instance)\n",
        "random_set = random.choices(triplets, k=TOTAL_QA)\n",
        "\n",
        "# Separate back to lists to use in give_an_answer function\n",
        "random_contexts  = []\n",
        "random_questions = []\n",
        "random_answers   = []\n",
        "\n",
        "for i in random_set:\n",
        "    random_contexts.append(i['context'])\n",
        "    random_questions.append(i['question'])\n",
        "    random_answers.append(i['answer'])"
      ],
      "metadata": {
        "id": "DEs6Y-nQ_ibv"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "totalF1 = 0\n",
        "\n",
        "for c, q, a in zip(random_contexts, random_questions, random_answers):\n",
        "    f = give_an_answer(c, q, a['text'])\n",
        "    totalF1 += f\n",
        "    print(\"\\n=========================\\n\")\n",
        "\n",
        "print(\"\\nF1 Score:\", totalF1/TOTAL_QA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJKiMcFk_iYy",
        "outputId": "1f7915bc-e73c-4aed-ef22-f0dbc3cd9760"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What does the Sieve of Eratosthenes do?\n",
            "Prediction: prime numbers :\n",
            "True Answer: compute primes\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: How many soldiers were in each Tumen?\n",
            "Prediction: 4\n",
            "True Answer: 10,000\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What future Revolutionary key figures participated in this attack?\n",
            "Prediction: thomas gage,\n",
            "True Answer: Washington and Thomas Gage\n",
            "EM: 0\n",
            "F1: 0.6666666666666666\n",
            "Precision: 1.0\n",
            "Recall: 0.5\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What alumni wrote \"The Good War\"?\n",
            "Prediction: \n",
            "True Answer: Studs Terkel\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What is the name of an algebraic structure in which addition, subtraction and multiplication are defined?\n",
            "Prediction: prime elements and irreducible elements. an element p of r is called prime element if it is neither zero nor a unit ( i. e., does not have a multiplicative inverse ) and satisfies the following requirement : given x and y in r such that p divides the product xy,\n",
            "True Answer: commutative ring R\n",
            "EM: 0\n",
            "F1: 0.04347826086956522\n",
            "Precision: 0.023255813953488372\n",
            "Recall: 0.3333333333333333\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: How many intercollegiate sports does Harvard compete in NCAA division I\n",
            "Prediction: 42\n",
            "True Answer: 42\n",
            "EM: 1\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What was the name of the leader through the Great Depression and World War II?\n",
            "Prediction: john harvard\n",
            "True Answer: James Bryant Conant\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: The party awarded a seat is the one with the highest what?\n",
            "Prediction: constituency seats )\n",
            "True Answer: quotient\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What is used to figure the relative strengh of gravity?\n",
            "Prediction: a dimensional constant is used to describe the relative strength of gravity. this constant has come to be known as newton ' s universal gravitation constant, though its value was unknown in newton ' s lifetime. not until 1798 was henry cavendish able to make the first measurement of using a torsion balance ;\n",
            "True Answer: dimensional constant\n",
            "EM: 0\n",
            "F1: 0.08163265306122448\n",
            "Precision: 0.0425531914893617\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Pressures greater than what can lead to convulsions?\n",
            "Prediction: 60 kpa can eventually lead to permanent pulmonary fibrosis. exposure to a o 2 partial pressures greater than 160 kpa ( about 1. 6 atm ) may lead to convulsions ( normally fatal for divers ). acute oxygen toxicity ( causing seizures, its most feared effect for divers ) can occur by breathing an air mixture with 21 % o 2 at 66 m or more of depth ;\n",
            "True Answer: 160 kPa\n",
            "EM: 0\n",
            "F1: 0.06557377049180328\n",
            "Precision: 0.03389830508474576\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What is the only form potential energy can change into?\n",
            "Prediction: an artifact of the potential field\n",
            "True Answer: kinetic\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What type of goals are usually done skirting the law?\n",
            "Prediction: harassment\n",
            "True Answer: propaganda\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What are engines using four expansion stages known as?\n",
            "Prediction: multiple expansion engine.\n",
            "True Answer: quadruple expansion engines\n",
            "EM: 0\n",
            "F1: 0.3333333333333333\n",
            "Precision: 0.3333333333333333\n",
            "Recall: 0.3333333333333333\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: The Hawaiian Islands are made up almost entirely of what?\n",
            "Prediction: layered basaltic lava flows.\n",
            "True Answer: layered basaltic lava flows\n",
            "EM: 1\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Who can enforce the European Union law when member states provide lesser rights?\n",
            "Prediction: the european commission\n",
            "True Answer: the courts of member states\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What country has the most illiterate adults in the world?\n",
            "Prediction: english\n",
            "True Answer: India\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What was the Yuan's unofficial state religion?\n",
            "Prediction: the bureau of buddhist and tibetan affairs ( xuanzheng yuan ) was set up in khanbaliq ( modern beijing ) to supervise buddhist monks throughout the empire. since kublai khan only esteemed the sakya sect of tibetan buddhism, other religions became less important. he and his successors kept a sakya imperial preceptor ( dishi ) at court. before the end of the yuan dynasty, 14 leaders of the sakya sect had held the post of imperial preceptor, thereby enjoying special power. furthermore, mongol patronage of buddhism resulted in a number of monuments of buddhist art. mongolian buddhist translations, almost all from tibetan originals, began on a large scale after 1300.\n",
            "True Answer: Tibetan Buddhism\n",
            "EM: 0\n",
            "F1: 0.04166666666666667\n",
            "Precision: 0.02127659574468085\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: imperialism often divides countries by using which technique?\n",
            "Prediction: diplomacy or military force.\n",
            "True Answer: othering\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: On what railroad was Salamanca used?\n",
            "Prediction: stockton and darlington railway.\n",
            "True Answer: Middleton Railway\n",
            "EM: 0\n",
            "F1: 0.3333333333333333\n",
            "Precision: 0.25\n",
            "Recall: 0.5\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Who inspects the building periodically to ensure that the construction adheres to the approved plans and the local building code?\n",
            "Prediction: the municipal building inspector\n",
            "True Answer: the municipal building inspector\n",
            "EM: 1\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: When did Sky launch a TV advertising campaign target towards women?\n",
            "Prediction: july 2007,\n",
            "True Answer: September 2007\n",
            "EM: 0\n",
            "F1: 0.5\n",
            "Precision: 0.5\n",
            "Recall: 0.5\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Thousands of madrasahs spawned what organization?\n",
            "Prediction: \n",
            "True Answer: The Taliban\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What type of ratios are used in geochronologic and thermochronologic studies?\n",
            "Prediction: \n",
            "True Answer: isotope\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Which country's arms purchase from the US became 5 times more than Israel?\n",
            "Prediction: \n",
            "True Answer: Saudi Arabia\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: When were theories developed suggesting inequality may have some positive effect on economic development?\n",
            "Prediction: \n",
            "True Answer: 1970s\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What was the Yuan's unofficial state religion?\n",
            "Prediction: the bureau of buddhist and tibetan affairs ( xuanzheng yuan ) was set up in khanbaliq ( modern beijing ) to supervise buddhist monks throughout the empire. since kublai khan only esteemed the sakya sect of tibetan buddhism, other religions became less important. he and his successors kept a sakya imperial preceptor ( dishi ) at court. before the end of the yuan dynasty, 14 leaders of the sakya sect had held the post of imperial preceptor, thereby enjoying special power. furthermore, mongol patronage of buddhism resulted in a number of monuments of buddhist art. mongolian buddhist translations, almost all from tibetan originals, began on a large scale after 1300.\n",
            "True Answer: Tibetan Buddhism\n",
            "EM: 0\n",
            "F1: 0.04166666666666667\n",
            "Precision: 0.02127659574468085\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What is used to quantify the intuitive undestanding of forces?\n",
            "Prediction: newtonian mechanics.\n",
            "True Answer: standard measurement scale\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What is the term given to algorithms that utilize random bits?\n",
            "Prediction: probabilistic turing machine\n",
            "True Answer: randomized algorithms\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Who led the Science and Environmental Policy Project?\n",
            "Prediction: fred singer ' s science and environmental policy project held a press event on capitol hill, washington, d. c., featuring comments on the graph wibjorn karlen and singer argued against the graph at a united states senate committee on commerce, science and transportation hearing on 18 july 2000. contrarian john lawrence daly featured a modified version of the ipcc 1990 schematic, which he mis - identified as appearing in the ipcc 1995 report, and argued that \" overturning its own previous view in the 1995 report, the ipcc presented the ' hockey stick ' as the new orthodoxy with hardly an apology or explanation for the abrupt u - turn since its 1995 report \". criticism of the mbh99 reconstruction in a review paper, which was quickly discredited in the soon and baliunas controversy, was picked up by the bush administration, and a senate speech by us republican senator james inhofe alleged that \" manmade global warming is the greatest hoax ever perpetrated on the american people \". the data and methodology used to produce the \" hockey stick graph \" was criticized in papers by stephen mcintyre and ross mckitrick, and in turn the criticisms in these papers were examined by other studies and comprehensively refuted by wahl & ammann 2007,\n",
            "True Answer: Fred Singer\n",
            "EM: 0\n",
            "F1: 0.0223463687150838\n",
            "Precision: 0.011299435028248588\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What experiences acceleration when external force is applied to a system?\n",
            "Prediction: \n",
            "True Answer: the center of mass\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Between Brakel and what other city can the most landward tidal influence be detected?\n",
            "Prediction: nijmegen,\n",
            "True Answer: Zaltbommel\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What did Robert Koch prove was the cause of infectious disease?\n",
            "Prediction: medicine\n",
            "True Answer: microorganisms\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: How many successors of Kublai was Toghun the last of?\n",
            "Prediction: nine\n",
            "True Answer: nine\n",
            "EM: 1\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Why was this short termed organization created?\n",
            "Prediction: to coordinate the response to the embargo.\n",
            "True Answer: to coordinate the response to the embargo\n",
            "EM: 1\n",
            "F1: 0.8000000000000002\n",
            "Precision: 0.8\n",
            "Recall: 0.8\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What did Lempicka represent better than anyone else?\n",
            "Prediction: art deco style in painting and art. nathan alterman, the israeli poet, was born in warsaw, as was moshe vilenski, the israeli composer, lyricist, and pianist, who studied music at the warsaw conservatory. warsaw was the beloved city of isaac bashevis singer,\n",
            "True Answer: Art Deco style\n",
            "EM: 0\n",
            "F1: 0.14634146341463414\n",
            "Precision: 0.07894736842105263\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: During what time period did income inequality decrease in the United States?\n",
            "Prediction: 1910 to 1940 and thereafter. [ citation needed ] however, recent data shows that the level of income inequality began to rise after the 1970s.\n",
            "True Answer: 1910 to 1940\n",
            "EM: 0\n",
            "F1: 0.25\n",
            "Precision: 0.14285714285714285\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Name a larger car that Toyota came up with as buyers lamented the small sized compacts?\n",
            "Prediction: \n",
            "True Answer: Corona Mark II\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Who created an engine using high pressure steam in 1801?\n",
            "Prediction: richard trevithick\n",
            "True Answer: Oliver Evans\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What type of skills does the market bid up compensation for?\n",
            "Prediction: rare and desired skills\n",
            "True Answer: rare and desired\n",
            "EM: 0\n",
            "F1: 0.8571428571428571\n",
            "Precision: 0.75\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: In what kind of system of particles are there no unbalanced iinternal forces?\n",
            "Prediction: closed\n",
            "True Answer: a closed system\n",
            "EM: 0\n",
            "F1: 0.6666666666666666\n",
            "Precision: 1.0\n",
            "Recall: 0.5\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: How did the Huguenots defend themselves?\n",
            "Prediction: trying to establish separate centers of power in southern france. retaliating against the french catholics, the huguenots had their own militia.\n",
            "True Answer: their own militia\n",
            "EM: 0\n",
            "F1: 0.2727272727272727\n",
            "Precision: 0.15789473684210525\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What do photocytes produce?\n",
            "Prediction: a system of internal canals. these branch through the mesoglea to the most active parts of the animal : the mouth and pharynx ;\n",
            "True Answer: bioluminescence\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What countries use a red stylized A to signify pharmacy?\n",
            "Prediction: the bowl of hygieia\n",
            "True Answer: Germany and Austria\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Where did John Paul II celebrate Mass in Warsaw?\n",
            "Prediction: victory square\n",
            "True Answer: Victory Square\n",
            "EM: 1\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What sorts of items are displayed in the Esteve Pharmacy museum?\n",
            "Prediction: old prescription books\n",
            "True Answer: old prescription books and antique drugs\n",
            "EM: 0\n",
            "F1: 0.6666666666666666\n",
            "Precision: 1.0\n",
            "Recall: 0.5\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: How far does one pharmacy in Croatia date back to?\n",
            "Prediction: 1422.\n",
            "True Answer: 1317\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What does critically tapered mean? \n",
            "Prediction: realistic - looking patterns of faulting and the growth of a critically tapered ( all angles remain the same ) orogenic wedge.\n",
            "True Answer: all angles remain the same\n",
            "EM: 0\n",
            "F1: 0.4\n",
            "Precision: 0.25\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What does Piketty feel was the biggest factors in reducing inequality between 1914 to 1945?\n",
            "Prediction: wars\n",
            "True Answer: wars and \"violent economic and political shocks\"\n",
            "EM: 0\n",
            "F1: 0.25\n",
            "Precision: 1.0\n",
            "Recall: 0.14285714285714285\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: There have been major changes in Amazon rainforest vegetation over the last how many years?\n",
            "Prediction: 21, 000 years\n",
            "True Answer: 21,000\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: The Sand Bernardino - Riverside area maintains what kind of district?\n",
            "Prediction: business districts\n",
            "True Answer: business\n",
            "EM: 0\n",
            "F1: 0.6666666666666666\n",
            "Precision: 0.5\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: How many bodies of water makes up Lake Constance?\n",
            "Prediction: three\n",
            "True Answer: three bodies of water\n",
            "EM: 0\n",
            "F1: 0.4\n",
            "Precision: 1.0\n",
            "Recall: 0.25\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What proposed attacks did Shirley plan?\n",
            "Prediction: an expedition through the wilderness of the maine district and down the chaudiere river\n",
            "True Answer: Fort Frontenac\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: How do you pronounce Fresno?\n",
            "Prediction: san joaquin valley\n",
            "True Answer: FREZ-noh\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Halford Mackinder and Friedrich Ratzel where what kind of geographers?\n",
            "Prediction: imperialism.\n",
            "True Answer: Political\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Which party is strongest in Victoria's northwestern and eastern regions?\n",
            "Prediction: \n",
            "True Answer: National Party\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Between Brakel and what other city can the most landward tidal influence be detected?\n",
            "Prediction: nijmegen,\n",
            "True Answer: Zaltbommel\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What was Fort Caroline renamed to after the Spanish attack?\n",
            "Prediction: the river of may because he discovered it in may. ribault erected a stone column near present - day jacksonville claiming the newly discovered land for france. in 1564, rene goulaine de laudonniere established the first european settlement, fort caroline, on the st. johns near the main village of the saturiwa. philip ii of spain ordered pedro menendez de aviles to protect the interest of spain by attacking the french presence at fort caroline. on september 20, 1565, a spanish force from the nearby spanish settlement of st. augustine attacked fort caroline, and killed nearly all the french soldiers defending it. the spanish renamed the fort san mateo,\n",
            "True Answer: fort San Mateo\n",
            "EM: 0\n",
            "F1: 0.0625\n",
            "Precision: 0.03225806451612903\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What did the number of legions in Roman times depend on?\n",
            "Prediction: eight\n",
            "True Answer: whether a state or threat of war existed\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: In what year did Harvard President Joseph Willard die?\n",
            "Prediction: 1803\n",
            "True Answer: 1804\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What type of locomotive was Salamanca?\n",
            "Prediction: twin - cylinder locomotive salamanca by matthew murray was used by the edge railed rack and pinion middleton railway. in 1825 george stephenson built the locomotion for the stockton and darlington railway.\n",
            "True Answer: twin-cylinder\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: How many seats are in the debating chamber?\n",
            "Prediction: 131\n",
            "True Answer: 131\n",
            "EM: 1\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: In which region tribe were large settlements discovered?\n",
            "Prediction: xingu tribe,\n",
            "True Answer: Xingu tribe\n",
            "EM: 1\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What is the longest river in Germany?\n",
            "Prediction: the rhine is the longest river in germany. it is here that the rhine encounters some more of its main tributaries, such as the neckar,\n",
            "True Answer: Rhine\n",
            "EM: 0\n",
            "F1: 0.0909090909090909\n",
            "Precision: 0.047619047619047616\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Which car shows the DOT's revision of categories?\n",
            "Prediction: mustang i\n",
            "True Answer: 1974 Mustang I\n",
            "EM: 0\n",
            "F1: 0.8\n",
            "Precision: 1.0\n",
            "Recall: 0.6666666666666666\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What were the Saxon Palace and Brühl Palace in prewar Warsaw?\n",
            "Prediction: socialist realism style\n",
            "True Answer: most distinctive buildings\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: How many passengers can the Ford Fiesta accommodate?\n",
            "Prediction: four\n",
            "True Answer: four\n",
            "EM: 1\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: In what way do idea strings transmit tesion forces?\n",
            "Prediction: instantaneously in action - reaction pairs\n",
            "True Answer: action-reaction pairs\n",
            "EM: 0\n",
            "F1: 0.28571428571428575\n",
            "Precision: 0.2\n",
            "Recall: 0.5\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Downtown Burbank is an example of what kind of district?\n",
            "Prediction: \n",
            "True Answer: major business districts\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: How many MSPs are there?\n",
            "Prediction: 129\n",
            "True Answer: 129\n",
            "EM: 1\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Who created an index of health and social problems?\n",
            "Prediction: unicef\n",
            "True Answer: authors Richard Wilkinson and Kate Pickett\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: When did BSkyB discontinue the Sky+ Box?\n",
            "Prediction: july 2007,\n",
            "True Answer: January 2010\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What flows between Bingen and Bonn?\n",
            "Prediction: \n",
            "True Answer: Middle Rhine\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Who was Jacksonville's mayor at the time of the consolidation?\n",
            "Prediction: hans tanzler\n",
            "True Answer: Hans Tanzler\n",
            "EM: 1\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What is the density of all primes compatible with a modulo 9?\n",
            "Prediction: what is the density of all primes compatible with a modulo 9? [SEP] can have infinitely many primes only when a and q are coprime,\n",
            "True Answer: 1/6\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What are the three primary expressions used to represent case complexity?\n",
            "Prediction: three primary expressions\n",
            "True Answer: best, worst and average case complexity\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Who noted the different current uses of civil disobedience?\n",
            "Prediction: marshall cohen notes, \" it has been used to describe everything from bringing a test - case in the federal courts to taking aim at a federal official. indeed, for vice president agnew\n",
            "True Answer: Marshall Cohen\n",
            "EM: 0\n",
            "F1: 0.13333333333333333\n",
            "Precision: 0.07142857142857142\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Data from physical experiments can be extrapolated to the field to understand what processes? \n",
            "Prediction: metamorphic processes\n",
            "True Answer: metamorphic\n",
            "EM: 0\n",
            "F1: 0.6666666666666666\n",
            "Precision: 0.5\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: When were the public housing developments built in the neighborhood?\n",
            "Prediction: between the 1960s and 1990s\n",
            "True Answer: between the 1960s and 1990s\n",
            "EM: 1\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: How much did the gross agricultural product increase from 2003-04?\n",
            "Prediction: 17 % to $ 8. 7 billion.\n",
            "True Answer: $8.7 billion\n",
            "EM: 0\n",
            "F1: 0.28571428571428575\n",
            "Precision: 0.2\n",
            "Recall: 0.5\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What had the Yuan used to print its money before bronze plates?\n",
            "Prediction: \n",
            "True Answer: woodblocks\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What character in the play portrays civil disobedience?\n",
            "Prediction: sophocles ' play antigone, in which antigone, one of the daughters of former king of thebes, oedipus, defies creon,\n",
            "True Answer: Antigone\n",
            "EM: 0\n",
            "F1: 0.1111111111111111\n",
            "Precision: 0.058823529411764705\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What sport attracts most tourists to Jacksonville?\n",
            "Prediction: golf.\n",
            "True Answer: golf\n",
            "EM: 1\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: By what process is singlet oxygen made in the tropophere?\n",
            "Prediction: photosynthesis,\n",
            "True Answer: photolysis\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Which phylum is more complex than sponges?\n",
            "Prediction: cnidarians\n",
            "True Answer: Ctenophores\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Who was Robert's son?\n",
            "Prediction: bohemond,\n",
            "True Answer: Bohemond\n",
            "EM: 1\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: In what meeting did Shirley lay out plans for 1756?\n",
            "Prediction: albany\n",
            "True Answer: Albany\n",
            "EM: 1\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Why did oil start getting priced in terms of gold?\n",
            "Prediction: gold? [SEP] on august 15, 1971, the united states unilaterally pulled out of the bretton woods accord.\n",
            "True Answer: Because oil was priced in dollars, oil producers' real income decreased.\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What is the Rhine called in Dutch?\n",
            "Prediction: * rinaz,\n",
            "True Answer: Rijn\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What does Article 102 of the Treaty of Lisbon prohibit?\n",
            "Prediction: price fixing.\n",
            "True Answer: the abuse of dominant position\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What element is used as a coolant in the process of making liquid oxygen?\n",
            "Prediction: liquid nitrogen\n",
            "True Answer: liquid nitrogen\n",
            "EM: 1\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: How many people died of plague in Paris in 1466?\n",
            "Prediction: 40, 000\n",
            "True Answer: 40,000\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What did Davies call the System \n",
            "Prediction: packet switching,\n",
            "True Answer: packet switching\n",
            "EM: 1\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: How many species of Ctenophora have been validated?\n",
            "Prediction: two species, which live as parasites on the salps on which adults of their species feed. in favorable circumstances, ctenophores can eat ten times their own weight in a day. only 100 – 150 species have been validated, and possibly another 25\n",
            "True Answer: 100–150 species\n",
            "EM: 0\n",
            "F1: 0.047619047619047616\n",
            "Precision: 0.025\n",
            "Recall: 0.5\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What was the result of the disobedience protesting the nuclear site?\n",
            "Prediction: arrest.\n",
            "True Answer: arrested\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What is the repulsive force of close range atom interaction?\n",
            "Prediction: pauli repulsion ( due to fermionic nature of electrons ) follows resulting in the force that acts in a direction normal to the surface interface between two objects. : 93 the normal force,\n",
            "True Answer: normal force\n",
            "EM: 0\n",
            "F1: 0.14285714285714288\n",
            "Precision: 0.07692307692307693\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What do most platyctenida have on their aboral surface?\n",
            "Prediction: comb - rows.\n",
            "True Answer: tentilla-bearing tentacles\n",
            "EM: 0\n",
            "F1: 0\n",
            "Precision: 0\n",
            "Recall: 0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Where was Ralph earl of?\n",
            "Prediction: wales. edward the confessor had set up the aforementioned ralph as earl of hereford\n",
            "True Answer: Hereford\n",
            "EM: 0\n",
            "F1: 0.15384615384615385\n",
            "Precision: 0.08333333333333333\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: When was King George's war?\n",
            "Prediction: 1740s,\n",
            "True Answer: 1740s\n",
            "EM: 1\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: What is the most commonplace model utilized in complexity theory?\n",
            "Prediction: a turing machine\n",
            "True Answer: the Turing machine\n",
            "EM: 1\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "Question: Who gave a negative connotation to civil disobedience in recent history?\n",
            "Prediction: vice president agnew\n",
            "True Answer: Vice President Agnew\n",
            "EM: 1\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n",
            "=========================\n",
            "\n",
            "\n",
            "F1 Score: 0.29286180430860226\n"
          ]
        }
      ]
    }
  ]
}